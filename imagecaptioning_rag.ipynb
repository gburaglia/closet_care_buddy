{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings #for converting chunks into embeddings\n",
    "from langchain_chroma import Chroma #database for stroring the embeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "from langchain_community.document_loaders import ImageCaptionLoader\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "# Dictionary to track temporary directories\n",
    "temp_dirs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2doc(img_path):\n",
    "    \n",
    "    # Create an ImageCaptionLoader instance\n",
    "    loader = ImageCaptionLoader(img_path)\n",
    "    # Load the caption as a document\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split the document into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    temp_dirs.append(temp_dir)  # Track temp directory for later cleanup\n",
    "    \n",
    "    # Create a new Chroma vectorstore\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=OpenAIEmbeddings(), \n",
    "        persist_directory=temp_dir\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(k=1)\n",
    "    \n",
    "    return retriever, temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashiondb(dir):\n",
    "    \"\"\"\n",
    "    dir is the directory of the vector DB\n",
    "    \"\"\"\n",
    "    embeddings_used = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorDB = Chroma(persist_directory=dir,embedding_function=embeddings_used)\n",
    "    retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_temp_dir(temp_dir):\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Temporary directory {temp_dir} cleaned up successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to clean up temporary directory {temp_dir}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gabrielaburaglia/Documents/GitHub/closet_care_buddy/chroma_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir = os.getcwd()\n",
    "db_dir = os.path.join(dir,\"chroma_db\")\n",
    "print(db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def textGeneration_langChain_RAG(img_path):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an experienced clothing sylist. \"\n",
    "        \"Use the following pieces of retrieved context to answer. \"\n",
    "        \"Use one sentence maximum and be as detailed as possible yet concise. \"\n",
    "        \"Include the clothing syle (i.e. bohemian, casual, classic, sporty, preppy). \"\n",
    "        \"Include how new or worn the item looks to be. \"\n",
    "        \"Be confident avoid using the word 'likely'. \"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    caption_retriever,temp_dir = img2doc(img_path)\n",
    "    reference_retriever = fashiondb(db_dir)\n",
    "\n",
    "    # initialize the ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[caption_retriever, reference_retriever], weights=[0.5, 0.5]\n",
    "    )\n",
    "    \n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(ensemble_retriever, question_answer_chain)\n",
    "\n",
    "    response = rag_chain.invoke({\"input\": \"Describe the piece of clothing in this picture\", \"context\": ensemble_retriever})\n",
    "\n",
    "    #rag_chain = prompt | llm | StrOutputParser()\n",
    "    cleanup_temp_dir(temp_dir)\n",
    "\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielaburaglia/Documents/GitHub/closet_care_buddy/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary directory /var/folders/d2/gls23ldn7jdgvvggh6p22l8c0000gn/T/tmpka_hulq1 cleaned up successfully.\n",
      "The image likely features a pair of cowboy boots, which are typically associated with a western style and appear to be new.\n"
     ]
    }
   ],
   "source": [
    "# Path to your image directory or image file\n",
    "res = textGeneration_langChain_RAG(\"static/imgs/hippie.png\")\n",
    "print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
